{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"dbt-airflow","text":"<p>A python package used to render dbt projects via Airflow DAGs. Every dbt resource type, including models, seeds,  snapshots and tests, will be assigned an individual task and the dependencies will automatically be inferred.  Additional tasks can also be added either before/after the entire dbt project, or in-between specific dbt tasks. </p>"},{"location":"#installation","title":"Installation","text":"<p>The package is available on PyPI:</p> <pre><code>pip install dbt-airflow\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>First, make sure that you have a fresh and up to date <code>manifest.json</code> metadata file that  is generated by <code>dbt</code> CLI (hint: See here the list of  commands that will generate one). </p> <pre><code>from datetime import datetime\nfrom pathlib import Path\n\nfrom airflow. import DAG\nfrom airflow.operators.dummy import DummyOperator\n\nfrom dbt_airflow.core.config import DbtAirflowConfig, DbtProjectConfig, DbtProfileConfig\nfrom dbt_airflow.core.task_group import DbtTaskGroup\nfrom dbt_airflow.operators.execution import ExecutionOperator\n\n\nwith DAG(\n    dag_id='test_dag',\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['example'],\n) as dag:\n\n    t1 = DummyOperator(task_id='dummy_1')\n    t2 = DummyOperator(task_id='dummy_2')\n\n    tg = DbtTaskGroup(\n        group_id='dbt-company',\n        dbt_project_config=DbtProjectConfig(\n            project_path=Path('/opt/airflow/example_dbt_project/'),\n            manifest_path=Path('/opt/airflow/example_dbt_project/target/manifest.json'),\n        ),\n        dbt_profile_config=DbtProfileConfig(\n            profiles_path=Path('/opt/airflow/example_dbt_project/profiles'),\n            target='dev',\n        ),\n        dbt_airflow_config=DbtAirflowConfig(\n            execution_operator=ExecutionOperator.BASH,\n        ),\n    )\n\n    t1 &gt;&gt; tg &gt;&gt; t2    \n</code></pre>"},{"location":"#things-to-know","title":"Things to know","text":"<p>Here's a list of some key aspects and assumptions of the implementation:</p> <ul> <li>Every dbt project, when compiled, will generate a metadata file under <code>&lt;dbt-project-dir&gt;/target/manifest.json</code></li> <li>The manifest file contains information about the interdependencies of the project's data models</li> <li><code>dbt-airflow</code> aims to extract these dependencies such that every dbt entity (snapshot, model, test and seed) has    its own task in a Airflow DAG while entity dependencies are persisted</li> <li>Snapshots are never an upstream dependency of any task</li> <li>The creation of snapshots on seeds does not make sense, and thus not handled (not even sure if this is even possible on dbt side)</li> <li>Models may have tests</li> <li>Snapshots may have tests</li> <li>Seeds may have tests</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>We are working hard to make <code>dbt-airflow</code> as generalised and customizable as possible, in order to  enable different users and teams serve different use-cases. The following sections, provide all the details you need in order to properly set the configuration up. </p> <p>Configuration is split into three categories and can be provided when creating a Task Group using <code>DbtTaskGroup</code>:  - dbt project configuration (<code>DbtProjectConfig</code>) - dbt profile configuration (<code>DbtProfileConfig</code>) - <code>dbt-airflow</code>-related configuration (<code>DbtAirflowConfig</code>)</p>"},{"location":"configuration/#dbt-project-configuration","title":"dbt project configuration","text":"<p>This type of configuration will help <code>dbt-airflow</code> determine the location of your dbt project and  the manifest file. It needs to be constructed using <code>dbt_airflow.core.config.DbtProjectConfig</code> and needs to be supplied via <code>dbt_project_config</code> argument in <code>DbtTaskGroup</code>. </p>"},{"location":"configuration/#parameters","title":"Parameters","text":"argument required type description <code>project_path</code> yes <code>pathlib.Path</code> The path to the dbt project (equivalent to dbt <code>--project-dir</code> flag) <code>manifest_path</code> yes <code>pathlib.Path</code> The path to the <code>manifest.json</code> file"},{"location":"configuration/#example","title":"Example","text":"<pre><code>from pathlib import Path \n\nfrom dbt_airflow.core.config import DbtProjectConfig\nfrom dbt_airflow.core.task_group import DbtTaskGroup\n\n\ntg = DbtTaskGroup(\n    ...,\n    dbt_project_config=DbtProjectConfig(\n        project_path=Path('/opt/airflow/example_dbt_project/'),\n        manifest_path=Path('/opt/airflow/example_dbt_project/target/manifest.json'),\n    ),\n)\n</code></pre>"},{"location":"configuration/#dbt-profile-configuration","title":"dbt profile configuration","text":"<p>This type of configuration will help <code>dbt-airflow</code> determine the dbt profile details it requires in order to run the dbt tasks on Airflow. It needs to be constructed using  <code>dbt_airflow.core.config.DbtProfileConfig</code> and must be supplied via <code>dbt_profile_config</code>  argument in <code>DbtTaskGroup</code>. </p>"},{"location":"configuration/#parameters_1","title":"Parameters","text":"argument required type description <code>profiles_path</code> yes <code>pathlib.Path</code> The path to the profiles path (equivalent to dbt <code>--profiles-dir</code> flag) <code>target</code> yes <code>str</code> The name of the target, as specified in <code>profiles.yml</code> file"},{"location":"configuration/#example_1","title":"Example","text":"<pre><code>from pathlib import Path \n\nfrom dbt_airflow.core.config import DbtProfileConfig\nfrom dbt_airflow.core.task_group import DbtTaskGroup\n\n\ntg = DbtTaskGroup(\n    ...,\n    dbt_profile_config=DbtProfileConfig(\n        profiles_path=Path('/opt/airflow/example_dbt_project/profiles'),\n        target='dev',\n    ),\n)\n</code></pre>"},{"location":"configuration/#dbt-airflow-configuration","title":"<code>dbt-airflow</code> configuration","text":"<p>This is an optional type of configuration and consists of numerous different settings that can help users customize the way they execute dbt projects on Airflow. It can be constructed using  <code>dbt_airflow.core.config.DbtAirflowConfig</code> and should be supplied via <code>dbt_airflow_config</code>  argument in <code>DbtTaskGroup</code>. </p>"},{"location":"configuration/#parameters_2","title":"Parameters","text":"argument type required default description <code>create_sub_task_groups</code> <code>bool</code> no <code>True</code> Boolean flag indicating whether sub TaskGroups will be created when rendering Airflow DAG. If <code>True</code>, the folder name of the dbt entity will be used as a <code>TaskGroup</code> <code>extra_tasks</code> <code>list</code> no <code>None</code> A list of <code>ExtraTask</code> objects, that can be used to introduce extra Airflow tasks within the rendered DAG. <code>execution_operator</code> <code>ExecutionOperator</code> no <code>ExecutionOperator.BASH</code> The execution operator for the dbt Airflow Tasks. <code>operator_kwargs</code> <code>dict</code> no <code>None</code> A dictionary with additional params/vals to be passed to the <code>execution_operator</code> <code>select</code> <code>list</code> no <code>None</code> Equivalent to <code>--select</code> flag of dbt CLI <code>exclude</code> <code>list</code> no <code>None</code> Equivalent to <code>--exclude</code> flag of dbt CLI <code>full_refresh</code> <code>bool</code> no <code>False</code> Equivalent to <code>--full-refresh</code> flag of dbt CLI <code>no_write_json</code> <code>bool</code> no <code>True</code> Equivalent to <code>--no-write-json</code> flag of dbt CLI <code>variables</code> <code>str</code> no <code>None</code> Equivalent to <code>--vars</code> flag of dbt CLI <code>no_partial_parse</code> <code>bool</code> no <code>False</code> Equivalent to <code>--no-partial-parse</code> flag of dbt CLI <code>warn_error</code> <code>bool</code> no <code>False</code> Equivalent to <code>--warn-error</code> flag of dbt CLI <code>warn_error_options</code> <code>str</code> no <code>None</code> Equivalent to <code>--warn-error-options</code> flag of dbt CLI <code>include_tags</code> <code>list</code> no <code>None</code> When specified, only dbt resources with these tags will be rendered on the Airflow DAG <code>exclude_tags</code> <code>list</code> no <code>None</code> When specified, dbt resources with these tags will not be rendered on the Airflow DAG <code>model_tasks_operator_kwargs</code> <code>dict</code> no <code>None</code> Operator Keyword arguments that will be supplied only for dbt model run tasks <code>test_tasks_operator_kwargs</code> <code>dict</code> no <code>None</code> Operator Keyword arguments that will be supplied only for dbt test tasks <code>seed_tasks_operator_kwargs</code> <code>dict</code> no <code>None</code> Operator Keyword arguments that will be supplied only for dbt seed tasks <code>snapshot_tasks_operator_kwargs</code> <code>dict</code> no <code>None</code> Operator Keyword arguments that will be supplied only for dbt snapshot tasks"},{"location":"contributing/","title":"Contributing to dbt-airflow","text":"<p>We encourage everyone to be part of this journey and contribute to the project. You can do so by implementing new features, fixing potential bugs or even improving our documentation and CI/CD pipelines. If you are planning to  contribute with the implementation of a new feature, you should first open an Issue on GitHub where you can share your ideas and designs so that project's contributors and maintainers can provide their insights. This will ensure that you won't be spending your time on implementing a piece of work that might not support the longer vision of the project and unavoidably might not be merged. </p>"},{"location":"contributing/#setting-up-a-local-environment","title":"Setting up a local environment","text":"<p>The next few sections will help you set up a local development environment where you can quickly test your changes,  before opening a Pull Request. </p>"},{"location":"contributing/#creating-a-fork","title":"Creating a fork","text":"<p>The first thing you need to do, is to create a fork of the repository. You can do so by clicking on the <code>Fork</code> button that is found on the top right corner of the project's page on GitHub. Once you create a fork, you'll then have to clone your fork onto your local machine. </p>"},{"location":"contributing/#setup-your-local-environment","title":"Setup your local environment","text":"<p>In your forked project's directory, create and activate a fresh virtual environment:</p> <pre><code># Create a virtual environment called `dbt-airflow-venv`\n$ python3 -m venv ~/dbt-airflow-venv\n\n# Activate the newly created virtual environment\n$ source ~/dbt-airflow-venv/bin/activate\n</code></pre> <p>Every single merge into <code>main</code> branch will trigger a new patch, minor or major version upgrade based on the commit  messages pushed  from the Pull Request. The automated release mechanism is based on  conventional commits. Every single commit must follow the structural elements described in Conventional Commits' specification.  The repository also contains pre-commit hooks that will ensure compliance to the specification.  Make sure to install pre-commit hooks to avoid any inconsistencies, by following the steps outlined below. </p> <pre><code># Install `pre-commit` package from PyPI\n$ python3 -m pip install pre-commit \n\n# Install hooks from `.pre-commit-config.yaml`\n$ pre-commit install\npre-commit installed at .git/hooks/pre-commit\n</code></pre>"},{"location":"contributing/#testing-your-changes-locally","title":"Testing your changes locally","text":"<p>In order to see how your changes will be reflected on Airflow, you can spin up the docker containers from the images specified in <code>docker-compose.yml</code> and <code>Dockerfile</code> files. </p> <pre><code># Build the images (you can omit `--no-cache` in case you don't want to re-build every layer)\n$ docker compose build --no-cache\n\n# Run the containers\n$ docker compose up\n</code></pre> <p>Basically, the commands above will spin up the following containers: - An Airflow instance whose webserver can be accessd on <code>localhost:8080</code> (use <code>airflow</code> and <code>airflow</code> in user/pass credentials) - A postgres instance containing the popular Sakila data, where dbt models can materialize - A container that gives you access to <code>dbt</code> CLI where you can run further <code>dbt</code> commands</p> <p>The Postgres instance can be accessed in the following way (note that default port was changed to <code>5433</code> given that we  have an additional postgres instance for Airflow itself):</p> <pre><code># Get the id of the running postgres-sakila container\n$ docker ps\n\n# Enter the running container\n$ docker exec -it &lt;container-id&gt; /bin/bash\n\n# Enter psql\n$ psql -U postgres -p 5433 \n</code></pre> <p>You will now be able to run Airflow DAGs authored with the use of <code>dbt-airflow</code> where you can also evaluate results either on the Airflow UI (webserver) or on the local database itself. </p> <p>Additionally, you need to make sure that all tests pass successfully. This project uses <code>poetry</code> tool for  dependency management. You'll have to install <code>poetry</code> and install the dependencies specified in <code>pyproject.toml</code>. </p> <pre><code># Install poetry\n$ python3 -m pip install poetry==1.3\n\n# Install dependencies\n$ poetry install\n</code></pre> <p>If you'd like to run the tests, make sure to do so within the poetry environment, as  shown below. </p> <pre><code># Run all tests\n$ poetry run pytest tests \n\n# Run test(s) with specific prefix or specific name\n$ poetry run pytest tests -k \"test_some_prefix_or_full_test_name\"\n</code></pre>"},{"location":"contributing/#opening-a-pull-request","title":"Opening a Pull Request","text":"<p>Once you have finished your local work, it's time to get it reviewed by project maintainers and other contributors. To do so, create a Pull Request from your fork into the original repository, <code>gmyrianthous/dbt-airflow</code>.</p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#populating-a-dbt-project-on-airflow-as-a-dag","title":"Populating a dbt project on Airflow, as a DAG","text":"<p>TODO: Write description</p> <pre><code>from datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom dbt_airflow.core.config import DbtProjectConfig, DbtProfileConfig\nfrom dbt_airflow.core.task_group import DbtTaskGroup\n\n\nwith DAG(\n    dag_id='test_dag',\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['example'],\n) as dag:\n\n    t1 = DummyOperator(task_id='dummy_1')\n    t2 = DummyOperator(task_id='dummy_2')\n    tg = DbtTaskGroup(\n        group_id='dbt-company',\n        dbt_project_config=DbtProjectConfig(\n            project_path=Path('/path/to/dbt/project/dir'),\n            manifest_path=Path('/path/to/target/manifest.json'),\n        ),\n        dbt_profile_config=DbtProfileConfig(\n            profiles_path=Path('/path/to/dbt/project/profiles/dir'),\n            target='dev',\n        )\n    )\n\n    t1 &gt;&gt; tg &gt;&gt; t2\n</code></pre>"},{"location":"examples/#airflow-dag-with-dbt-project-and-additional-dependencies","title":"Airflow DAG with dbt project and additional dependencies","text":"<p>TODO</p>"},{"location":"execution_operators/","title":"Running dbt-airflow using different operators","text":"<p><code>dbt-airflow</code> currently supports two Airflow Operators: - <code>BashOperator</code> (default) - <code>KubernetesPodOperator</code></p> <p>The package will automatically render your project as an Airflow DAG consisting of Airflow Tasks for each dbt resource. The constructed Airflow Tasks will use the corresponding operator based on  your selection for <code>execution_operator</code> that can be provided as part of the <code>DbtAirflowConfig</code>  object. Furthermore, you can also optionally provide further operator-specific arguments through the  <code>operator_kwargs</code> argument. </p>"},{"location":"execution_operators/#running-dbt-projects-with-bashoperator","title":"Running dbt projects with <code>BashOperator</code>","text":"<p>This is the default execution operator and every dbt resource will be rendered on the Airflow DAG as a <code>BashOperator</code>.</p> <p>When using <code>ExecutionOperator.BASH</code> it is assumed that <code>dbt-core</code> (and the corresponding dbt  adapter) are installed already on Airflow. Sometimes this is not possible given that there's a  high chance of hitting the wall of conflicting package versions. If this is the case, then you need to consider building your own image with your dbt project and the relevant dependencies, and run <code>dbt-airflow</code> using <code>ExecutionOperator.KUBERNETES</code>. </p>"},{"location":"execution_operators/#example-dag-with-bashoperator","title":"Example DAG with <code>BashOperator</code>","text":"<pre><code>from datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import DAG\n\nfrom dbt_airflow.core.config import DbtAirflowConfig, DbtProjectConfig, DbtProfileConfig\nfrom dbt_airflow.core.task_group import DbtTaskGroup\nfrom dbt_airflow.operators.execution import ExecutionOperator\n\n\nwith DAG(\n    dag_id='test_dag',\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['example'],\n) as dag:\n\n    tg = DbtTaskGroup(\n        group_id='dbt-company',\n        dbt_project_config=DbtProjectConfig(\n            project_path=Path('/opt/airflow/example_dbt_project/'),\n            manifest_path=Path('/opt/airflow/example_dbt_project/target/manifest.json'),\n        ),\n        dbt_profile_config=DbtProfileConfig(\n            profiles_path=Path('/opt/airflow/example_dbt_project/profiles'),\n            target='dev',\n        ),\n        dbt_airflow_config=DbtAirflowConfig(\n            execution_operator=ExecutionOperator.BASH,\n        ),\n    )\n</code></pre>"},{"location":"execution_operators/#running-dbt-projects-with-kubernetespodoperator","title":"Running dbt projects with <code>KubernetesPodOperator</code>","text":"<p>An alternative way to render dbt projects on Airflow via <code>dbt-airflow</code> is the use of  <code>KubernetesPodOperator</code>. This execution operator, assumes that you have previously created a Docker Image consisting of your dbt project as well as the relevant dependencies (including <code>dbt-core</code> and dbt adapter). </p> <p>In order to run your project in this mode you will have to specify  <code>execution_operator=ExecutionOperator.BASH</code> in <code>DbtAirflowConfig</code> and specify required and optional arguments for <code>KubernetesPodOperator</code> via <code>operator_kwargs</code> argument. </p>"},{"location":"execution_operators/#example-dag-with-kubernetespodoperator","title":"Example DAG with <code>KubernetesPodOperator</code>","text":"<pre><code>from datetime import datetime\nfrom pathlib import Path\n\nfrom airflow import DAG\n\nfrom dbt_airflow.core.config import DbtAirflowConfig, DbtProjectConfig, DbtProfileConfig\nfrom dbt_airflow.core.task_group import DbtTaskGroup\nfrom dbt_airflow.operators.execution import ExecutionOperator\n\n\nwith DAG(\n    dag_id='test_dag',\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['example'],\n) as dag:\n\n    tg = DbtTaskGroup(\n        group_id='dbt-company',\n        dbt_project_config=DbtProjectConfig(\n            project_path=Path('/opt/airflow/example_dbt_project/'),\n            manifest_path=Path('/opt/airflow/example_dbt_project/target/manifest.json'),\n        ),\n        dbt_profile_config=DbtProfileConfig(\n            profiles_path=Path('/opt/airflow/example_dbt_project/profiles'),\n            target='dev',\n        ),\n        dbt_airflow_config=DbtAirflowConfig(\n            execution_operator=ExecutionOperator.KUBERNETES,\n            operator_kwargs={\n                'name': f'dbt-company',\n                'namespace': 'composer-user-workloads',\n                'image': 'ghcr.io/dbt-labs/dbt-bigquery:1.7.2',\n                'kubernetes_conn_id': 'kubernetes_default',\n                'config_file': '/home/airflow/composer_kube_config',\n                'image_pull_policy': 'Always',\n            },\n        ),\n    )\n</code></pre>"}]}